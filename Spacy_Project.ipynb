{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxedvn4yLCb2"
   },
   "source": [
    "# Machine learning entity recognition with Spacy\n",
    "\n",
    "The aim of this notebook is going to be to present the main components of spacy, and the way it can be used for NLP(*Natural Language Processing*) and NER(*Named Entity Recognition*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8vXpKdo6B-6"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "from traindata import TRAIN_DATA\n",
    "from testdata import test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uENyZZIGLEfF"
   },
   "source": [
    "# What is a NER?\n",
    "\n",
    "A named entity is a ”real-world object” that’s assigned a name; for example, a person, a country, a\n",
    "product or a book title. A NER is the problem of automatically recognizing these entities from the text.\n",
    "\n",
    "For our case, our model will be trained in order to recognise Machine Learning related entities, which are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir2TmF-uhvFo"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USED ENTITIES\n",
    "SML = \"SUPERVISED MACHINE LEARNING ALGORITHMS\" \n",
    "USML = \"UNSUPERVISED MACHINE LEARNING ALGORITHMS\"\n",
    "MLS = \"MACHINE LEARNING SOFTWARE\"\n",
    "NN = \"NEURAL NETWORKS\"\n",
    "EVM = \"EVATUATION METHODS\" \n",
    "OPM = \"OPTIMIZATION METHODS\" \n",
    "MLP = \"MACHINE LEARNING PREPROCESSING\" \n",
    "MLA = \"MACHINE LEARNING APPLICATIONS\" \n",
    "\"\"\"\n",
    "\n",
    "LABELS = [\"EVM\",\"MLA\",\"MLP\",\"MLS\",\"NN\",\"OPM\",\"SML\",\"USML\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H18wfA_3H0m3"
   },
   "source": [
    "# How does spaCy use the data?\n",
    "\n",
    "Here some training data is defined. The mode data is presented to the model is the following one:\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    (\n",
    "        \"We introduced a multilayer perceptron neural network (MLPNN) based classification model as a diagnostic decision support mechanism in the epilepsy treatment.\",\n",
    "        {\"entities\": [(27, 37, \"NN\")]}\n",
    "    )\n",
    "    ,\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    ,\n",
    "    (\n",
    "        \"The main issue in gradient descent is: how should we set the step size?\",\n",
    "        {\"entities\": [(18, 34, \"OPM\")]}\n",
    "    )\n",
    "]\n",
    "   \n",
    "```\n",
    "\n",
    "As it is shown, each sample of data is formed by 3 main components:\n",
    "\n",
    "\n",
    "*   The text which is going to be used.\n",
    "*   The \"entities\" label, which indicates de entities inside the text.\n",
    "*   For each entity that is into the text, this will be represented giving its first character, its last one and the entity related to that sequence of characters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMy1t5gRoalK"
   },
   "source": [
    "That whole training set is an array of 245 of those kind of samples. That is because we used 7 samples of text to train each entity keyword and there are 35 keywords defined.\n",
    "\n",
    "Although each keyword on our set has its own 7 training text samples, some of them could appear on other keywords' samples. So each keyword doesn't necesarily only have to appear only on 7 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lz-J2d0iP9yJ"
   },
   "source": [
    "After defining the test data and the parameters which are going to be given to the model, the model is set.\n",
    "\n",
    "For that, the first step is to define and set a pipeline for the model. Once the pipe is set, the labels we have chosen are given to the model.\n",
    "\n",
    "The following step is to start and give the training data to the model.\n",
    "\n",
    "After being trained, the test data is given, and the predictions are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJq77vFePSU3"
   },
   "outputs": [],
   "source": [
    "def ner(model=None, new_model_name=\"machine_learning\", output_dir=None,\n",
    "         n_iter=30, no_train=False, train=TRAIN_DATA, test=test_text,\n",
    "         display=False, verbose=True):\n",
    "    \n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "        \n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    for label in LABELS:\n",
    "        ner.add_label(label)  # add new entity label to entity recognizer\n",
    "\n",
    "    if no_train is False:\n",
    "        if model is None:\n",
    "            optimizer = nlp.begin_training()\n",
    "        else:\n",
    "            optimizer = nlp.resume_training()\n",
    "        move_names = list(ner.move_names)\n",
    "        # get names of other pipes to disable them during training\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "        with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "            sizes = compounding(1.0, 4.0, 1.001)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            for itn in range(n_iter):\n",
    "                random.shuffle(train)\n",
    "                batches = minibatch(train, size=sizes)\n",
    "                losses = {}\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "                if verbose:\n",
    "                    print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    doc = nlp(test)\n",
    "    if verbose:\n",
    "        print(\"Entities in '%s'\" % test)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "    \n",
    "    if display:\n",
    "      displacy.render(doc, style=\"ent\")\n",
    "\n",
    "        \n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test)\n",
    "        for i in range(len(doc2.ents)):\n",
    "            print(doc2.ents[i].label_, doc2.ents[i].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqEFNNLVc9fo"
   },
   "source": [
    "Now, with the *ner* function we can search for entities on a given piece of text.\n",
    "\n",
    "To show how the algorithm performs, here we have 3 test datasets to test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kW7olmp9XDm"
   },
   "source": [
    "## Test Data 1 \n",
    "\n",
    "bibliography: https://arxiv.org/abs/1603.04467\n",
    "\n",
    "Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are \n",
    "reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7NR2FCWSkzr"
   },
   "source": [
    "True Results:\n",
    "\n",
    "\n",
    "EVM accuracy\n",
    "\n",
    "MLA pattern recognition\n",
    "\n",
    "NN Convolutional neural networks\n",
    "\n",
    "NN CNN\n",
    "\n",
    "NN CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GJYgV-bmSBC"
   },
   "outputs": [],
   "source": [
    "test_data1 = \"Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR4xT-eYmO0u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 683.4059019869624}\n",
      "Losses {'ner': 502.16813123820083}\n",
      "Losses {'ner': 516.5445525329308}\n",
      "Losses {'ner': 383.4062892875651}\n",
      "Losses {'ner': 406.7460062252648}\n",
      "Losses {'ner': 395.65957051832754}\n",
      "Losses {'ner': 238.2972186868646}\n",
      "Losses {'ner': 241.82102537283455}\n",
      "Losses {'ner': 154.90342826756088}\n",
      "Losses {'ner': 157.53743602455177}\n",
      "Losses {'ner': 126.72646123833393}\n",
      "Losses {'ner': 127.5487532796723}\n",
      "Losses {'ner': 103.67188086076496}\n",
      "Losses {'ner': 85.86203695173698}\n",
      "Losses {'ner': 69.00787480475861}\n",
      "Losses {'ner': 61.76679128620278}\n",
      "Losses {'ner': 70.36747670356141}\n",
      "Losses {'ner': 55.17540647737548}\n",
      "Losses {'ner': 42.04819482826178}\n",
      "Losses {'ner': 51.78060115235919}\n",
      "Losses {'ner': 41.47540000290169}\n",
      "Losses {'ner': 30.02583450758375}\n",
      "Losses {'ner': 30.67997645859194}\n",
      "Losses {'ner': 57.700090934355096}\n",
      "Losses {'ner': 43.05153185557546}\n",
      "Losses {'ner': 32.067996120803095}\n",
      "Losses {'ner': 18.837938587331397}\n",
      "Losses {'ner': 21.052852233171475}\n",
      "Losses {'ner': 39.719600571452865}\n",
      "Losses {'ner': 20.510489378414313}\n",
      "Entities in 'Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons.'\n",
      "EVM accuracy\n",
      "MLA pattern recognition\n",
      "EVM reported\n",
      "NN Convolutional neural networks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    accuracy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVM</span>\n",
       "</mark>\n",
       ". Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    pattern recognition\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MLA</span>\n",
       "</mark>\n",
       " problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    reported\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVM</span>\n",
       "</mark>\n",
       " and the best-performing algorithms are briefly described. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Convolutional neural networks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NN</span>\n",
       "</mark>\n",
       " (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner(test=test_data1, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JBEEbKP9qYY"
   },
   "source": [
    "## Test Data 2\n",
    "\n",
    "bibliography: https://www.sciencedirect.com/science/article/abs/pii/S0893608012000524\n",
    "\n",
    "We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.\n",
    "\n",
    "True Results: it should not detect any entity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CpCa_-InaRI"
   },
   "outputs": [],
   "source": [
    "test_data2 = \"We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YaCdPVQndwI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 773.8040341823641}\n",
      "Losses {'ner': 518.2561895478623}\n",
      "Losses {'ner': 510.74411564770134}\n",
      "Losses {'ner': 473.45823618790524}\n",
      "Losses {'ner': 413.0625895038406}\n",
      "Losses {'ner': 326.6295818049622}\n",
      "Losses {'ner': 309.35498028165375}\n",
      "Losses {'ner': 239.39783058905294}\n",
      "Losses {'ner': 218.64945087878874}\n",
      "Losses {'ner': 154.09937041041846}\n",
      "Losses {'ner': 132.809166232902}\n",
      "Losses {'ner': 121.1173487412944}\n",
      "Losses {'ner': 103.95665659366922}\n",
      "Losses {'ner': 62.24152791426415}\n",
      "Losses {'ner': 66.59271001888239}\n",
      "Losses {'ner': 59.69104201657272}\n",
      "Losses {'ner': 68.42556428107255}\n",
      "Losses {'ner': 48.03789954268608}\n",
      "Losses {'ner': 63.92999775133345}\n",
      "Losses {'ner': 45.52803203126494}\n",
      "Losses {'ner': 26.4302666521596}\n",
      "Losses {'ner': 20.03063847110248}\n",
      "Losses {'ner': 41.1804953381024}\n",
      "Losses {'ner': 23.836054179640925}\n",
      "Losses {'ner': 39.86883502827105}\n",
      "Losses {'ner': 40.25180307324177}\n",
      "Losses {'ner': 18.98684155493404}\n",
      "Losses {'ner': 24.402821979894163}\n",
      "Losses {'ner': 23.675399277930456}\n",
      "Losses {'ner': 16.22061368558726}\n",
      "Entities in 'We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.'\n",
      "MLA boosts recognition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    boosts recognition\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MLA</span>\n",
       "</mark>\n",
       " performance, making the system insensitive also to variations in contrast and illumination.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner(test=test_data2, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoWM1o0aAazU"
   },
   "source": [
    "\n",
    "## Test data 3\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0377221719307581\n",
    "\n",
    "## Sample 1\n",
    "\n",
    "Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.\n",
    "\n",
    "True Results:\n",
    "\n",
    "SML random forest\n",
    "\n",
    "SML support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyuwkXXHniGc"
   },
   "outputs": [],
   "source": [
    "test_data3 = \"Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBKBHUexniTe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 705.553140829905}\n",
      "Losses {'ner': 554.2483269347199}\n",
      "Losses {'ner': 439.4455183279283}\n",
      "Losses {'ner': 392.1363177653914}\n",
      "Losses {'ner': 377.02664412544016}\n",
      "Losses {'ner': 319.94063104402966}\n",
      "Losses {'ner': 292.340112187175}\n",
      "Losses {'ner': 329.0608547936795}\n",
      "Losses {'ner': 212.31664854824}\n",
      "Losses {'ner': 248.6126177221075}\n",
      "Losses {'ner': 202.64447807742937}\n",
      "Losses {'ner': 120.82670855404132}\n",
      "Losses {'ner': 121.56290537528159}\n",
      "Losses {'ner': 150.0824573068004}\n",
      "Losses {'ner': 133.65968443412342}\n",
      "Losses {'ner': 102.89023267600622}\n",
      "Losses {'ner': 70.57187362424028}\n",
      "Losses {'ner': 88.76385886270172}\n",
      "Losses {'ner': 57.55155543511073}\n",
      "Losses {'ner': 62.1374046718277}\n",
      "Losses {'ner': 43.009942288648745}\n",
      "Losses {'ner': 49.801021237717215}\n",
      "Losses {'ner': 51.02923547224943}\n",
      "Losses {'ner': 30.691979923617637}\n",
      "Losses {'ner': 33.66686487736281}\n",
      "Losses {'ner': 23.268300897113658}\n",
      "Losses {'ner': 52.83792033528988}\n",
      "Losses {'ner': 34.88742009944289}\n",
      "Losses {'ner': 14.79367238067407}\n",
      "Losses {'ner': 45.34913993525497}\n",
      "Losses {'ner': 28.03913859930199}\n",
      "Losses {'ner': 25.240228786746176}\n",
      "Losses {'ner': 19.887860140138667}\n",
      "Losses {'ner': 26.804253113261097}\n",
      "Losses {'ner': 14.325569315009174}\n",
      "Losses {'ner': 13.55993788217609}\n",
      "Losses {'ner': 10.796471916653074}\n",
      "Losses {'ner': 15.020075764122609}\n",
      "Losses {'ner': 25.025561448734138}\n",
      "Losses {'ner': 11.403719495182495}\n",
      "Losses {'ner': 8.86345376736669}\n",
      "Losses {'ner': 15.087202870651097}\n",
      "Losses {'ner': 29.089647394335238}\n",
      "Losses {'ner': 20.197705132662986}\n",
      "Losses {'ner': 19.676154388143928}\n",
      "Losses {'ner': 7.3927193452262285}\n",
      "Losses {'ner': 23.565070268424414}\n",
      "Losses {'ner': 31.637265235605813}\n",
      "Losses {'ner': 11.618127599825044}\n",
      "Losses {'ner': 24.646817741373184}\n",
      "Losses {'ner': 24.296678217519457}\n",
      "Losses {'ner': 23.181246132742487}\n",
      "Losses {'ner': 14.614118283800131}\n",
      "Losses {'ner': 5.8794763592864845}\n",
      "Losses {'ner': 11.15426375230324}\n",
      "Losses {'ner': 18.489427844500668}\n",
      "Losses {'ner': 8.304779198319416}\n",
      "Losses {'ner': 6.246958290327509}\n",
      "Losses {'ner': 11.896531723655597}\n",
      "Losses {'ner': 14.434548941920504}\n",
      "Entities in 'Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.'\n",
      "SML random forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    random forest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SML</span>\n",
       "</mark>\n",
       ", artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner(test=test_data3, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcn1PbYPuJi8"
   },
   "source": [
    "# Iterations\n",
    "\n",
    "We can also specify how many learning steps we want to let the ner algorithm to perform. The default is 30.\n",
    "\n",
    "With this we can make the model more or less generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoqjlqDJucks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 769.1856485565568}\n",
      "Losses {'ner': 517.0872431410693}\n",
      "Losses {'ner': 503.3544283852964}\n",
      "Losses {'ner': 429.2526813369054}\n",
      "Losses {'ner': 314.8621959970043}\n",
      "Losses {'ner': 267.1537978904657}\n",
      "Losses {'ner': 185.5939556734168}\n",
      "Losses {'ner': 193.72961720384188}\n",
      "Losses {'ner': 233.95218292936556}\n",
      "Losses {'ner': 147.46257670226055}\n",
      "Losses {'ner': 120.3743944709245}\n",
      "Losses {'ner': 139.67006263467715}\n",
      "Losses {'ner': 98.70510378411329}\n",
      "Losses {'ner': 125.82624417980153}\n",
      "Losses {'ner': 82.90958383556752}\n",
      "Entities in 'Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.'\n",
      "SML random forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    random forest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SML</span>\n",
       "</mark>\n",
       ", artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner(test=test_data3, display=True, n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jDEBSbmu1EG"
   },
   "source": [
    "# Other options\n",
    "\n",
    "The next 4 parameters are disabled by default:\n",
    "\n",
    "*   An original model can be used with *model* parameter.\n",
    "*   The model generated with the training data and, possibly, the model provided as a parameter, can be saved to be used later with *output_dir* parameter.\n",
    "*   It can be specified not to train the model using the train data with *no_train* parameter.\n",
    "*   We used this option previusly: *display*. When this set, displaCy module is used to show NER results in a more visual and graphical way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9076ViexSbQ"
   },
   "source": [
    "All the following examples will have displaCy module active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9eqR2mqxWhs"
   },
   "source": [
    "This will save the model to a folder called 'model_saved' on the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1rUtAtfxTCI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 757.7506059012806}\n",
      "Losses {'ner': 479.8928215445575}\n",
      "Losses {'ner': 463.38400331127855}\n",
      "Losses {'ner': 519.7771797129587}\n",
      "Losses {'ner': 369.00485377589735}\n",
      "Losses {'ner': 264.27750366838416}\n",
      "Losses {'ner': 243.41312621867684}\n",
      "Losses {'ner': 167.39341862923348}\n",
      "Losses {'ner': 190.41872648994325}\n",
      "Losses {'ner': 171.8223864435501}\n",
      "Losses {'ner': 132.00731378448305}\n",
      "Losses {'ner': 118.1058399069023}\n",
      "Losses {'ner': 132.1097551817286}\n",
      "Losses {'ner': 78.1121502385319}\n",
      "Losses {'ner': 89.21295682021456}\n",
      "Entities in 'Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.'\n",
      "SML random forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    random forest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SML</span>\n",
       "</mark>\n",
       ", artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to model_saved\n",
      "Loading from model_saved\n",
      "SML random forest\n"
     ]
    }
   ],
   "source": [
    "ner(test=test_data3, display=True, output_dir=\"model_saved\", n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hSjZbHyxTVW"
   },
   "source": [
    "This will use 'model_saved' as input model. Which does not mean it will not have to train again the model with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cHLnl1-xTqE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'model_saved'\n",
      "Losses {'ner': 121.9760862417908}\n",
      "Losses {'ner': 113.96285160290937}\n",
      "Losses {'ner': 166.37370022802315}\n",
      "Losses {'ner': 148.82273698699805}\n",
      "Losses {'ner': 127.59486012951297}\n",
      "Losses {'ner': 117.22499197867774}\n",
      "Losses {'ner': 144.65114638474927}\n",
      "Losses {'ner': 87.04298682047589}\n",
      "Losses {'ner': 66.3504866443029}\n",
      "Losses {'ner': 83.7146834552211}\n",
      "Losses {'ner': 64.1496275232343}\n",
      "Losses {'ner': 46.56448164254833}\n",
      "Losses {'ner': 52.06255387813236}\n",
      "Losses {'ner': 18.869948108420093}\n",
      "Losses {'ner': 28.853743164465758}\n",
      "Entities in 'Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.'\n",
      "SML random forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    random forest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SML</span>\n",
       "</mark>\n",
       ", artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner(test=test_data3, display=True, model=\"model_saved\", n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhLIhN9HxT60"
   },
   "source": [
    "Here 'model_saved' is used again, but the model will not be trainined using training data, so the execution should be very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StB9GlOIxUPS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'model_saved'\n",
      "Entities in 'Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as random forest, artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.'\n",
      "SML random forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Deep learning is essential in the context of big data and, for this purpose, its performance across different scenarios is compared and contrasted in this overview article. The empirical results of the different cases studies suggest that deep learning is a feasible and effective method, which can considerably and consistently outperform its traditional counterparts in both prediction and operational performance from the family of data-analytic models. As such, DNNs are able identify previously unknown, potentially useful, non-trivial, and interesting patterns more accurately than other popular predictive models such as \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    random forest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SML</span>\n",
       "</mark>\n",
       ", artificial neural networks, and support vector machines. One of the reasons they yield superior results originates from the strong mathematical assumptions in traditional machine learning, whereas these are relaxed by DNNs as a result of their larger parameter space.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner(test=test_data3, display=True, no_train=True, model=\"model_saved\", n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 0\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 1\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 2\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 3\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 4\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML DNNs are\n",
      "SML random forest\n",
      "SML support vector machines\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML DNNs are\n",
      "SML random forest\n",
      "SML support vector machines\n",
      "\n",
      "\n",
      "ITERATION: 5\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 6\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 7\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 8\n",
      "Loaded model 'model_saved2'\n",
      "MLP purpose\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "MLP purpose\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 9\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 10\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 11\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 12\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 13\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 14\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 15\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 16\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 17\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "USML DNNs\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "USML DNNs\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 18\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 19\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 20\n",
      "Loaded model 'model_saved2'\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 21\n",
      "Loaded model 'model_saved2'\n",
      "MLP purpose\n",
      "SML random forest\n",
      "Saved model to model_saved1\n",
      "Loading from model_saved1\n",
      "MLP purpose\n",
      "SML random forest\n",
      "Loaded model 'model_saved1'\n",
      "SML random forest\n",
      "Saved model to model_saved2\n",
      "Loading from model_saved2\n",
      "SML random forest\n",
      "\n",
      "\n",
      "ITERATION: 22\n",
      "Loaded model 'model_saved2'\n"
     ]
    }
   ],
   "source": [
    "ner(test=test_data3, display=False, output_dir=\"model_saved2\", verbose=False)\n",
    "for i in range (53):\n",
    "    print(\"\\n\\nITERATION: \" + str(i))\n",
    "    ner(test=test_data3, display=False, model=\"model_saved2\", output_dir=\"model_saved1\", verbose=False)\n",
    "    shutil.rmtree(\"model_saved2\")\n",
    "    ner(test=test_data3, display=False, model=\"model_saved1\", output_dir=\"model_saved2\", verbose=False)\n",
    "    shutil.rmtree(\"model_saved1\")\n",
    "    \n",
    "ner(test=test_data1, display=True, model=\"model_saved2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"model_saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stiFarLwn52K"
   },
   "source": [
    "# Cross Validation\n",
    "\n",
    "In order to to test the overall performance of our algorithm, we have used cross validation.\n",
    "\n",
    "As mentioned before, we trained the model with 245 samples, split in 35 groups of 7 samples for a specific keyword. That means that cross validation cannot be applied in any way, as the condition to find a keyword on a given text, is to have learned about that keyword before.\n",
    "\n",
    "For example, if we used Leave-one-out cross validation (LOOCV), the keyword on its sample of text used for validation in each iteration of the process would only be found because the other 6 samples of text used for learning that. However, the rest of samples on the training set would not be used as they have no information about the keyword cross validation is trying to find on each iteration. So, for each keyword, only samples related to it are relevant (generally 7).\n",
    "\n",
    "That said, what we are using is Leave-p-out cross validation, where on this case: ```p = 35``` (leave one sample of text out for each keyword)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bKFXFJbUyFK"
   },
   "outputs": [],
   "source": [
    "def getFold(traindata, fold):\n",
    "    return [traindata[i][0] for i in range(len(traindata)) if i%7 == fold]\n",
    "\n",
    "def getNotFold(traindata, fold):\n",
    "    return [traindata[i] for i in range(len(traindata)) if i%7 != fold]\n",
    "\n",
    "def concatByNewLine(strings):\n",
    "    res = \"\"\n",
    "    if len(strings) > 0:\n",
    "        res = strings[0]\n",
    "        for s in strings[1:]:\n",
    "            res += \"\\n\" + s\n",
    "    return res\n",
    "\n",
    "def crossvalidation(TRAIN_DATA, iterations=30):\n",
    "    print(\"\\n\\nITERATIONS = \" + str(iterations))\n",
    "    traindata = copy.deepcopy(TRAIN_DATA)\n",
    "  \n",
    "    for i in range(7):\n",
    "        print(\"\\nIteration: \" + str(i))\n",
    "        test = concatByNewLine(getFold(TRAIN_DATA, i))\n",
    "        train = getNotFold(TRAIN_DATA, i)\n",
    "        ner(n_iter=iterations, train=train, test=test)\n",
    "        print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK2KG-VutgIm"
   },
   "source": [
    "In order to use cross validation we just run the next cell with the whole training set with 245 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tsdavc1i0F0-"
   },
   "outputs": [],
   "source": [
    "crossvalidation(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pExhgBp9x9jK"
   },
   "source": [
    "Number of iterations can be set using *iterations* parameter. Default is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5idEb5U1_88p"
   },
   "outputs": [],
   "source": [
    "crossvalidation(TRAIN_DATA, iterations=15)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Spacy_Project.ipynb",
   "provenance": [
    {
     "file_id": "1VvoC5pa3LH2BgvvPQluVRcoWswOw8mxw",
     "timestamp": 1571485107262
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
