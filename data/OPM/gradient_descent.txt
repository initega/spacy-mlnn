
Perhaps the simplest algorithm for unconstrained optimization is gradient descent, also known
as steepest descent.

The main issue in gradient descent is: how should we set the step size?

This can be used inside a (stochastic) gradient descent procedure, discussed in Section 8.5.2.

As it stands, WARP loss is still hard to optimize, but it can be further
approximated by Monte Carlo sampling, and then optimized by gradient descent, as described.

It is straightforward to derive a gradient descent algorithm to fit this model; however, it
is rather slow.

Then sketch how to use projected gradient descent to solve this problem.

Since the Netflix data is so large (about 100 million observed entries), it is common
to use stochastic gradient descent (Section 8.5.2) for this task.